package repositories

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"log"
	"math/rand"
	"sort"
	"strconv"
	"time"

	"github.com/3Eeeecho/go-clouddisk/internal/models"
	"github.com/3Eeeecho/go-clouddisk/internal/pkg/cache"
	"github.com/3Eeeecho/go-clouddisk/internal/pkg/logger"
	"github.com/3Eeeecho/go-clouddisk/internal/pkg/mapper"
	"github.com/3Eeeecho/go-clouddisk/internal/pkg/xerr"
	"github.com/go-redis/redis/v8"
	"go.uber.org/zap"
	"gorm.io/gorm"
)

// FileRepository å®šä¹‰æ–‡ä»¶æ•°æ®è®¿é—®å±‚æ¥å£
type FileRepository interface {
	Create(file *models.File) error

	FindByID(id uint64) (*models.File, error)
	//FindByUserID(userID uint64) ([]models.File, error)                                          // è·å–ç”¨æˆ·æ‰€æœ‰æ–‡ä»¶
	FindByUserIDAndParentFolderID(userID uint64, parentFolderID *uint64) ([]models.File, error) // è·å–æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ–‡ä»¶
	FindByPath(path string) (*models.File, error)
	FindByUUID(uuid string) (*models.File, error)     // æ ¹æ® UUID æŸ¥æ‰¾
	FindByOssKey(ossKey string) (*models.File, error) //æ ¹æ® OssKey æŸ¥æ‰¾
	FindByFileName(userID uint64, parentFolderID *uint64, fileName string) (*models.File, error)
	FindFileByMD5Hash(md5Hash string) (*models.File, error)        // æ ¹æ®å­˜å‚¨è·¯å¾„æŸ¥æ‰¾æ–‡ä»¶
	FindDeletedFilesByUserID(userID uint64) ([]models.File, error) //æŸ¥æ‰¾å›æ”¶ç«™ä¸­çš„æ–‡ä»¶
	FindChildrenByPathPrefix(userID uint64, pathPrefix string) ([]models.File, error)
	CountFilesInStorage(ossKey string, md5Hash string, excludeFileID uint64) (int64, error)

	UpdateFilesPathInBatch(userID uint64, oldPathPrefix, newPathPrefix string) error
	Update(file *models.File) error
	SoftDelete(id uint64) error          // è½¯åˆ é™¤æ–‡ä»¶
	PermanentDelete(fileID uint64) error // æ°¸ä¹…åˆ é™¤æ–‡ä»¶
	UpdateFileStatus(fileID uint64, status uint8) error

	getFilesFromCacheList(ctx context.Context, listCacheKey string) ([]models.File, error)
	saveFilesToCacheList(ctx context.Context, cacheKey string, files []models.File, scoreFunc func(file models.File) float64) error
}

type fileRepository struct {
	db    *gorm.DB
	cache *cache.RedisCache
}

// NewFileRepository åˆ›å»ºä¸€ä¸ªæ–°çš„ FileRepository å®ä¾‹
func NewFileRepository(db *gorm.DB, cache *cache.RedisCache) FileRepository {
	return &fileRepository{
		db:    db,
		cache: cache,
	}
}

func (r *fileRepository) Create(file *models.File) error {
	err := r.db.Create(file).Error
	if err != nil {
		logger.Error("Create: Failed to create file in DB", zap.Error(err), zap.Uint64("userID", file.UserID), zap.String("fileName", file.FileName))
		return fmt.Errorf("failed to create file: %w", err)
	}
	ctx := context.Background()
	pipe := r.cache.TxPipeline()
	// å°†æ–°æ–‡ä»¶çš„å…ƒæ•°æ®å­˜å…¥ file:metadata:<new_file_id>
	fileMetadataKey := cache.GenerateFileMetadataKey(file.ID)
	fileMap, err := mapper.FileToMap(file) // è¾…åŠ©å‡½æ•°å°† models.File æ˜ å°„åˆ° map[string]any
	if err != nil {
		logger.Error("FindByID: Failed to map models.File to hash for caching", zap.Uint64("id", file.ID), zap.Error(err))
	} else {
		pipe.HMSet(ctx, fileMetadataKey, fileMap)
		// æ·»åŠ éšæœºçš„åç§»é‡,é˜²æ­¢å¤§é‡ç¼“å­˜è¿‡æœŸ(ç¼“å­˜é›ªå´©)
		pipe.Expire(ctx, fileMetadataKey, cache.CacheTTL+time.Duration(rand.Intn(300))*time.Second)
	}

	// ZAdd å°†æ–°æ–‡ä»¶çš„ ID åŠå…¶ CreatedAt çš„ Unix æ—¶é—´æˆ³ä½œä¸º Scoreï¼Œæ·»åŠ åˆ°å¯¹åº”çš„ Sorted Set ä¸­
	listCacheKey := cache.GenerateFileListKey(file.UserID, file.ParentFolderID)
	pipe.ZAdd(ctx, listCacheKey, &redis.Z{
		Score:  float64(file.CreatedAt.Unix()),
		Member: strconv.FormatUint(file.ID, 10),
	})

	//å¦‚æœä¹‹å‰å­˜è¿‡"__EMPTY_LIST__",éœ€è¦ZRemæ‰
	pipe.ZRem(ctx, listCacheKey, "__EMPTY_LIST__")

	if _, execErr := pipe.Exec(ctx); execErr != nil {
		logger.Error("Create: Failed to execute Redis pipeline for cache update",
			zap.Uint64("fileID", file.ID),
			zap.Uint64("userID", file.UserID),
			zap.Error(execErr),
		)
		// ç¼“å­˜æ›´æ–°å¤±è´¥é€šå¸¸ä¸è¿”å›é”™è¯¯ï¼Œä½†éœ€è¦è®°å½•
	}
	logger.Info("Create: File created and cache updated", zap.Uint64("fileID", file.ID), zap.Uint64("userID", file.UserID))
	return nil
}

func (r *fileRepository) FindByID(id uint64) (*models.File, error) {
	ctx := context.Background()
	fileMetadataKey := cache.GenerateFileMetadataKey(id)

	// å°è¯•ä»Redisç¼“å­˜ä¸­è·å–
	// å•æ–‡ä»¶ç¼“å­˜é‡‡ç”¨Hashç»“æ„
	resultMap, err := r.cache.HGetAll(ctx, fileMetadataKey)
	if err == nil {
		if _, ok := resultMap["__NOT_FOUND__"]; ok {
			return nil, xerr.ErrFileNotFound //  å¦‚æœä»ç¼“å­˜å‘½ä¸­ä¸å­˜åœ¨æ ‡è®°ï¼Œç›´æ¥è¿”å›ä¸å­˜åœ¨é”™è¯¯
		}
		file, err := mapper.MapToFile(resultMap) // è¾…åŠ©å‡½æ•°å°† map[string]string æ˜ å°„åˆ° models.File
		if err == nil {
			return file, nil
		}
		logger.Error("FindByID: Failed to map cached hash to models.File", zap.Uint64("id", id), zap.Error(err))
	} else if !errors.Is(err, cache.ErrCacheMiss) { // åªæœ‰ä¸æ˜¯ ErrCacheMiss æ‰è®°å½•é”™è¯¯
		logger.Error("FindByID: Error getting file hash from cache", zap.Uint64("id", id), zap.Error(err))
	}

	// ç¼“å­˜æœªå‘½ä¸­æˆ–è·å–å¤±è´¥ï¼Œä»æ•°æ®åº“ä¸­åŠ è½½
	var file models.File
	err = r.db.Unscoped().First(&file, id).Error
	if err != nil {
		if errors.Is(err, gorm.ErrRecordNotFound) {
			// å¦‚æœæ•°æ®åº“ä¸­ä¹Ÿä¸å­˜åœ¨ï¼Œç¼“å­˜ä¸€ä¸ªç©ºå€¼ï¼Œé˜²æ­¢ç¼“å­˜ç©¿é€
			r.cache.HSet(ctx, fileMetadataKey, "__NOT_FOUND__", "1")
			r.cache.Expire(ctx, fileMetadataKey, 1*time.Minute)
			return nil, xerr.ErrFileNotFound // æ–‡ä»¶æœªæ‰¾åˆ°
		}
		return nil, fmt.Errorf("file not found: %w", err)
	}

	// å°†ä»æ•°æ®åº“ä¸­è·å–çš„æ•°æ®å­˜å…¥ Redis ç¼“å­˜
	fileMap, err := mapper.FileToMap(&file) // è¾…åŠ©å‡½æ•°å°† models.File æ˜ å°„åˆ° map[string]any
	if err != nil {
		logger.Error("FindByID: Failed to map models.File to hash for caching", zap.Uint64("id", id), zap.Error(err))
	} else {
		r.cache.HMSet(ctx, fileMetadataKey, fileMap)                                                   // ä½¿ç”¨å°è£…å¥½çš„ HMSet
		r.cache.Expire(ctx, fileMetadataKey, cache.CacheTTL+time.Duration(rand.Intn(300))*time.Second) // ä½¿ç”¨å°è£…å¥½çš„ Expire
	}

	return &file, nil
}

// è·å–æŒ‡å®šç”¨æˆ·åœ¨ç‰¹å®šçˆ¶æ–‡ä»¶å¤¹ä¸‹çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹
// parentFolderID å¯ä»¥ä¸º nilï¼Œè¡¨ç¤ºæ ¹ç›®å½•
func (r *fileRepository) FindByUserIDAndParentFolderID(userID uint64, parentFolderID *uint64) ([]models.File, error) {
	ctx := context.Background()

	// å°è¯•ä»Redisç¼“å­˜ä¸­è·å–
	var files []models.File
	listCacheKey := cache.GenerateFileListKey(userID, parentFolderID)

	files, err := r.getFilesFromCacheList(ctx, listCacheKey)
	if err == nil {
		// ç¼“å­˜å‘½ä¸­ï¼Œå¹¶ä¸”æ•°æ®å·²è·å–å¹¶ååºåˆ—åŒ–
		// å¯¹è·å–åˆ°çš„æ–‡ä»¶åˆ—è¡¨æ’åº
		sort.Slice(files, func(i, j int) bool {
			// ä¼˜å…ˆæ˜¾ç¤ºæ–‡ä»¶å¤¹ (IsFolder=1åœ¨å‰)
			if files[i].IsFolder != files[j].IsFolder {
				return files[i].IsFolder > files[j].IsFolder // 1 > 0 => folder comes before file
			}
			// å…¶æ¬¡æŒ‰æ–‡ä»¶åæ’åº
			return files[i].FileName < files[j].FileName // ASC order
		})
		return files, nil
	} else if !errors.Is(err, cache.ErrCacheMiss) { // åªæœ‰ä¸æ˜¯ ErrCacheMiss æ‰è®°å½•é”™è¯¯
		logger.Error("FindByUserIDAndParentFolderID: Error getting file list from cache", zap.String("key", listCacheKey), zap.Error(err))
	}

	var dbFiles []models.File
	query := r.db.Where("user_id = ?", userID)

	if parentFolderID == nil {
		query = query.Where("parent_folder_id IS NULL") // æŸ¥æ‰¾æ ¹ç›®å½•
	} else {
		query = query.Where("parent_folder_id = ?", *parentFolderID) // æŸ¥æ‰¾æŒ‡å®šæ–‡ä»¶å¤¹
	}

	// ä¼˜å…ˆæ˜¾ç¤ºæ–‡ä»¶å¤¹ï¼Œç„¶åæŒ‰æ–‡ä»¶åæ’åº
	err = query.Order("is_folder DESC, file_name ASC").Find(&dbFiles).Error
	if err != nil {
		logger.Error("Error finding files from DB", zap.Uint64("userID", userID), zap.Any("parentFolderID", parentFolderID), zap.Error(err))
		return nil, fmt.Errorf("failed to find files: %w", err)
	}

	saveErr := r.saveFilesToCacheList(ctx, listCacheKey, dbFiles, func(file models.File) float64 {
		return float64(file.CreatedAt.Unix())
	})
	if saveErr != nil {
		logger.Error("FindByUserIDAndParentFolderID: Failed to save files to cache", zap.Error(saveErr))
		// å³ä½¿ç¼“å­˜å¤±è´¥ï¼Œä¹Ÿè¿”å›ä»æ•°æ®åº“æŸ¥åˆ°çš„æ•°æ®ï¼Œä¸é˜»å¡ä¸šåŠ¡
	}
	return dbFiles, nil
}

// FindFileByMD5Hash æ ¹æ® MD5Hash æŸ¥æ‰¾æ–‡ä»¶
func (r *fileRepository) FindFileByMD5Hash(md5Hash string) (*models.File, error) {
	ctx := context.Background()
	fileMetadataKey := cache.GenerateFileMD5Key(md5Hash)
	// å°è¯•ä»Redisç¼“å­˜ä¸­è·å–
	resultMap, err := r.cache.HGetAll(ctx, fileMetadataKey)
	if err == nil {
		if _, ok := resultMap["__NOT_FOUND__"]; ok {
			return nil, xerr.ErrFileNotFound //  å¦‚æœä»ç¼“å­˜å‘½ä¸­ä¸å­˜åœ¨æ ‡è®°ï¼Œç›´æ¥è¿”å›ä¸å­˜åœ¨é”™è¯¯
		}
		file, err := mapper.MapToFile(resultMap) // è¾…åŠ©å‡½æ•°å°† map[string]string æ˜ å°„åˆ° models.File
		if err == nil {
			return file, nil
		}
		logger.Error("FindByID: Failed to map cached hash to models.File", zap.String("md5Hash", md5Hash), zap.Error(err))
	} else if !errors.Is(err, cache.ErrCacheMiss) { // åªæœ‰ä¸æ˜¯ ErrCacheMiss æ‰è®°å½•é”™è¯¯
		logger.Error("FindByID: Error getting file hash from cache", zap.String("md5Hash", md5Hash), zap.Error(err))
	}

	var file models.File
	// æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å¯èƒ½éœ€è¦æŸ¥è¯¢çš„æ˜¯é‚£äº›éæ–‡ä»¶å¤¹ä¸”çŠ¶æ€æ­£å¸¸çš„æ–‡ä»¶çš„ MD5Hash
	err = r.db.Where("md5_hash = ? AND is_folder = 0 AND status = 1", md5Hash).First(&file).Error
	if err != nil {
		if errors.Is(err, gorm.ErrRecordNotFound) {
			// å¦‚æœæ•°æ®åº“ä¸­ä¹Ÿä¸å­˜åœ¨ï¼Œç¼“å­˜ä¸€ä¸ªç©ºå€¼ï¼Œé˜²æ­¢ç¼“å­˜ç©¿é€
			r.cache.HSet(ctx, fileMetadataKey, "__NOT_FOUND__", "1")
			r.cache.Expire(ctx, fileMetadataKey, 1*time.Minute)
			return nil, xerr.ErrFileNotFound // æ–‡ä»¶æœªæ‰¾åˆ°
		}
		log.Printf("Error finding file by MD5 hash %s: %v", md5Hash, err)
		return nil, err
	}

	// å°†ä»æ•°æ®åº“ä¸­è·å–çš„æ•°æ®å­˜å…¥ Redis ç¼“å­˜
	fileMap, err := mapper.FileToMap(&file) // è¾…åŠ©å‡½æ•°å°† models.File æ˜ å°„åˆ° map[string]any
	if err != nil {
		logger.Error("FindByID: Failed to map models.File to hash for caching", zap.String("md5Hash", md5Hash), zap.Error(err))
	} else {
		r.cache.HMSet(ctx, fileMetadataKey, fileMap)                                                   // ä½¿ç”¨å°è£…å¥½çš„ HMSet
		r.cache.Expire(ctx, fileMetadataKey, cache.CacheTTL+time.Duration(rand.Intn(300))*time.Second) // ä½¿ç”¨å°è£…å¥½çš„ Expire
	}

	return &file, nil
}

func (r *fileRepository) FindDeletedFilesByUserID(userID uint64) ([]models.File, error) {
	ctx := context.Background()
	// å°è¯•ä»Redisç¼“å­˜ä¸­è·å–
	listCacheKey := cache.GenerateDeletedFilesKey(userID)

	files, err := r.getFilesFromCacheList(ctx, listCacheKey)
	if err == nil {
		// è¿™é‡Œçš„æ’åºé€šå¸¸æ˜¯ DeletedAt DESCï¼ŒZRevRange å·²ç»èƒ½ä¿è¯è¿™ä¸ªé¡ºåº
		sort.Slice(files, func(i, j int) bool {
			return files[i].DeletedAt.Time.After(files[j].DeletedAt.Time)
		})
		logger.Info("successfully hit cache")
		return files, nil
	} else if !errors.Is(err, cache.ErrCacheMiss) {
		logger.Error("FindDeletedFilesByUserID: Error getting deleted file list from cache", zap.String("key", listCacheKey), zap.Error(err))
	}

	var dbFiles []models.File
	err = r.db.Unscoped().Where("user_id = ?", userID).Where("deleted_at IS NOT NULL").Order("deleted_at DESC").Find(&dbFiles).Error
	if err != nil {
		logger.Error("Error finding deleted files from DB", zap.Uint64("userID", userID), zap.Error(err))
		return nil, fmt.Errorf("æŸ¥è¯¢å·²åˆ é™¤æ–‡ä»¶åˆ—è¡¨å¤±è´¥: %w", err)
	}
	logger.Info("DB Query Result (after Find)", zap.Int("count", len(dbFiles)), zap.Uint64("userID", userID))
	for i, file := range dbFiles {
		logger.Info("DB File", zap.Int("index", i), zap.Uint64("id", file.ID), zap.Timep("deleted_at", &file.DeletedAt.Time), zap.String("filename", file.FileName))
	}

	// å°†ä»æ•°æ®åº“ä¸­è·å–çš„æ•°æ®å­˜å…¥ Redis ç¼“å­˜
	saveErr := r.saveFilesToCacheList(ctx, listCacheKey, dbFiles, func(file models.File) float64 {
		score := float64(0)
		if file.DeletedAt.Valid {
			score = float64(file.DeletedAt.Time.Unix())
		}
		return score
	})
	if saveErr != nil {
		logger.Error("FindDeletedFilesByUserID: Failed to save deleted files to cache", zap.Error(saveErr))
	}

	return dbFiles, nil
}

// æœªä½¿ç”¨å‡½æ•°å‡æ²¡è®¾ç½®redisç›¸å…³çš„é€»è¾‘,ä¹Ÿæ²¡æœ‰å†updateå‡½æ•°ä¸­æ·»åŠ åˆ é™¤ç¼“å­˜é€»è¾‘
// è·å–ç”¨æˆ·æ‰€æœ‰æ–‡ä»¶ (ä¸åŒ…æ‹¬æ–‡ä»¶å¤¹ï¼Œæˆ–è€…å¯ä»¥æ ¹æ®IsFolderè¿‡æ»¤) (æœªä½¿ç”¨)
func (r *fileRepository) FindByUserID(userID uint64) ([]models.File, error) {
	ctx := context.Background()

	// å°è¯•ä»Redisç¼“å­˜ä¸­è·å–
	var files []models.File
	cacheKey := fmt.Sprintf("file:user_id:%d", userID)
	err := r.cache.Get(ctx, cacheKey, &files)
	if err == nil {
		return files, nil
	} else {
		logger.Error("è·å–ç¼“å­˜æ•°æ®å‘ç”Ÿé”™è¯¯", zap.Uint64("id", userID), zap.Error(err))
	}

	// æŸ¥è¯¢ç”¨æˆ·æ‰€æœ‰æ–‡ä»¶å’Œæ–‡ä»¶å¤¹ï¼ŒæŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œä¸åŒ…æ‹¬å·²åˆ é™¤çš„
	err = r.db.Where("user_id = ?", userID).Order("created_at desc").Find(&files).Error
	if err != nil {
		log.Printf("Error finding files for user %d: %v", userID, err)
		return nil, err
	}

	// å°†ä»æ•°æ®åº“ä¸­è·å–çš„æ•°æ®å­˜å…¥ Redis ç¼“å­˜
	r.cache.Set(ctx, cacheKey, files, cache.CacheTTL+time.Duration(rand.Intn(300))*time.Second)

	return files, nil
}

// FindByUUID æ ¹æ® UUID æŸ¥æ‰¾æ–‡ä»¶ (æœªä½¿ç”¨)
func (r *fileRepository) FindByUUID(uuid string) (*models.File, error) {
	ctx := context.Background()

	// å°è¯•ä»Redisç¼“å­˜ä¸­è·å–
	var file models.File
	cacheKey := fmt.Sprintf("file:uuid:%s", uuid)
	err := r.cache.Get(ctx, cacheKey, &file)
	if err == nil {
		return &file, nil
	} else {
		logger.Error("è·å–æ–‡ä»¶ç¼“å­˜æ•°æ®å‘ç”Ÿé”™è¯¯", zap.String("uuid", uuid), zap.Error(err))
	}

	// ä»æ•°æ®åº“ä¸­è·å–æ•°æ®
	err = r.db.Where("uuid = ?", uuid).First(&file).Error
	if err != nil {
		log.Printf("Error finding file by UUID %s: %v", uuid, err)
		return nil, err
	}

	// å°†ä»æ•°æ®åº“ä¸­è·å–çš„æ•°æ®å­˜å…¥ Redis ç¼“å­˜
	r.cache.Set(ctx, cacheKey, file, cache.CacheTTL+time.Duration(rand.Intn(300))*time.Second)

	return &file, nil
}

// FindByOssKey æ ¹æ® OssKey æŸ¥æ‰¾æ–‡ä»¶ (æœªä½¿ç”¨)
func (r *fileRepository) FindByOssKey(ossKey string) (*models.File, error) {
	ctx := context.Background()

	// å°è¯•ä»Redisç¼“å­˜ä¸­è·å–
	var file models.File
	cacheKey := fmt.Sprintf("file:ossKey:%s", ossKey)
	err := r.cache.Get(ctx, cacheKey, &file)
	if err == nil {
		return &file, nil
	} else {
		logger.Error("è·å–æ–‡ä»¶ç¼“å­˜æ•°æ®å‘ç”Ÿé”™è¯¯", zap.String("ossKey", ossKey), zap.Error(err))
	}

	err = r.db.Where("oss_key = ?", ossKey).First(&file).Error
	if err != nil {
		log.Printf("Error finding file by OssKey %s: %v", ossKey, err)
		return nil, err
	}

	// å°†ä»æ•°æ®åº“ä¸­è·å–çš„æ•°æ®å­˜å…¥ Redis ç¼“å­˜
	r.cache.Set(ctx, cacheKey, file, cache.CacheTTL+time.Duration(rand.Intn(300))*time.Second)

	return &file, nil
}

// TODO ç¼“å­˜é€»è¾‘
func (r *fileRepository) FindByFileName(userID uint64, parentFolderID *uint64, fileName string) (*models.File, error) {
	var file models.File
	query := r.db.Where("user_id = ? AND file_name = ?", userID, fileName)
	if parentFolderID == nil {
		query = query.Where("parent_folder_id IS NULL")
	} else {
		query = query.Where("parent_folder_id = ?", *parentFolderID)
	}
	err := query.First(&file).Error
	return &file, err
}

// æ ¹æ®å­˜å‚¨è·¯å¾„æŸ¥æ‰¾æ–‡ä»¶
// ç¼“å­˜å¤±æ•ˆé€»è¾‘éå¸¸å¤æ‚ï¼Œç»´æŠ¤æˆæœ¬é«˜,æš‚æ—¶ä¸è€ƒè™‘æ·»åŠ ç¼“å­˜é€»è¾‘
func (r *fileRepository) FindByPath(path string) (*models.File, error) {
	var file models.File
	err := r.db.Where("storage_path = ?", path).First(&file).Error
	if err != nil {
		log.Printf("Error finding file by path %s: %v", path, err)
		return nil, err
	}
	return &file, nil
}

func (r *fileRepository) Update(file *models.File) error {
	// è·å–æ–‡ä»¶æ—§çŠ¶æ€ï¼Œç”¨äºåˆ¤æ–­ ParentFolderID æ˜¯å¦å˜åŒ–
	oldFile, findErr := r.FindByID(file.ID)
	if findErr != nil && !errors.Is(findErr, gorm.ErrRecordNotFound) { // å¦‚æœæ˜¯å…¶ä»–æŸ¥æ‰¾é”™è¯¯
		logger.Error("Update: Failed to retrieve old file state for cache invalidation", zap.Uint64("fileID", file.ID), zap.Error(findErr))
		// è¿™é‡Œçš„é”™è¯¯åº”è¯¥å‘ä¸Šè¿”å›ï¼Œå› ä¸ºå®ƒå¯èƒ½å½±å“æ›´æ–°é€»è¾‘
		return fmt.Errorf("failed to get old file state for update: %w", findErr)
	}
	// å¦‚æœ oldFile ä¸º nil (gorm.ErrRecordNotFound) è¡¨ç¤ºæ–‡ä»¶ä¸å­˜åœ¨ï¼Œç›´æ¥è¿”å›é”™è¯¯
	// è¿™é‡Œå‡è®¾ Save ä¼šæ ¹æ® ID æ›´æ–°ï¼Œå¦‚æœ ID ä¸å­˜åœ¨ï¼ŒSave ä¹Ÿä¼šæŠ¥é”™ã€‚
	// æ‰€ä»¥å¦‚æœ FindByID è¿”å› NotFoundï¼Œé€šå¸¸ Update ä¹Ÿä¸åº”è¯¥è¿›è¡Œã€‚
	if errors.Is(findErr, gorm.ErrRecordNotFound) || oldFile == nil {
		return fmt.Errorf("Update: file with ID %d not found in DB", file.ID)
	}

	err := r.db.Save(file).Error
	if err != nil {
		logger.Error("Update: Failed to update file in DB", zap.Error(err), zap.Uint64("fileID", file.ID), zap.Uint64("userID", file.UserID))
		return fmt.Errorf("failed to update file: %w", err)
	}

	ctx := context.Background()

	// åœ¨å‘é€å¼‚æ­¥æ¶ˆæ¯å‰ï¼Œç«‹å³ã€åŒæ­¥åœ°åˆ é™¤å½“å‰æ–‡ä»¶çš„å…ƒæ•°æ®ç¼“å­˜
	// è¿™å¯ä»¥ç¡®ä¿åç»­çš„è¯»è¯·æ±‚ä¼šå‘ç”Ÿç¼“å­˜æœªå‘½ä¸­ï¼Œä»è€Œç›´æ¥ä»æ•°æ®åº“è¯»å–æœ€æ–°æ•°æ®ï¼Œé¿å…æ•°æ®ä¸ä¸€è‡´
	fileMetadataKey := cache.GenerateFileMetadataKey(file.ID)
	if err := r.cache.Del(ctx, fileMetadataKey); err != nil {
		// å³ä½¿åˆ é™¤ç¼“å­˜å¤±è´¥ï¼Œä¹Ÿåªè®°å½•æ—¥å¿—ï¼Œä¸é˜»å¡ä¸»æµç¨‹ï¼Œå› ä¸ºå¼‚æ­¥æ¶ˆè´¹è€…æœ€ç»ˆä¼šå¤„ç†
		logger.Error("Update: Failed to synchronously delete file metadata cache", zap.Uint64("fileID", file.ID), zap.Error(err))
	}

	message := cache.CacheUpdateMessage{
		File:              *file,
		OldParentFolderID: oldFile.ParentFolderID,
		OldMD5Hash:        oldFile.MD5Hash,
		OldDeletedAt:      oldFile.DeletedAt,
	}
	messageJSON, _ := json.Marshal(message)

	// 4. å°†æ¶ˆæ¯å‘é€åˆ° Redis Streams
	_, streamErr := r.cache.XAdd(ctx, &redis.XAddArgs{
		Stream: "file_cache_updates", // Redis Stream çš„åç§°
		MaxLen: 10000,                // é™åˆ¶é˜Ÿåˆ—é•¿åº¦
		Values: map[string]any{
			"payload": messageJSON, // å°† JSON å­—èŠ‚æµä½œä¸º payload
		},
	}).Result()

	if streamErr != nil {
		// ğŸš¨ æ¶ˆæ¯å‘é€å¤±è´¥ä¸è¿”å›é”™è¯¯ï¼Œä½†å¿…é¡»è®°å½•æ—¥å¿—å¹¶è§¦å‘å‘Šè­¦
		logger.Error("Update: Failed to publish cache update message",
			zap.Uint64("fileID", file.ID),
			zap.Error(streamErr))
		// âš ï¸ æ³¨æ„ï¼šè¿™é‡Œä¸ return errï¼Œå› ä¸ºæ•°æ®åº“å·²æ›´æ–°æˆåŠŸï¼Œåªè®°å½•å¤±è´¥
	}

	return err
}

// è½¯åˆ é™¤æ–‡ä»¶,è®¾ç½®DeletedAtå­—æ®µ
func (r *fileRepository) SoftDelete(fileID uint64) error {
	file, err := r.FindByID(fileID) // è¿™é‡Œ FindByID ä¼šä»ç¼“å­˜æˆ–DBè·å–ï¼Œä»¥è·å–æ–‡ä»¶è¯¦ç»†ä¿¡æ¯
	if err != nil {
		return err
	}
	if file == nil {
		return xerr.ErrFileNotFound
	}

	// è½¯åˆ é™¤æ–‡ä»¶
	if err = r.db.Model(file).
		Where("id = ?", fileID).
		Updates(map[string]any{
			"deleted_at": time.Now(), // æ˜¾å¼è®¾ç½® deleted_atï¼Œä»¥é˜² GORM ç‰ˆæœ¬è¡Œä¸ºä¸ä¸€è‡´
			"status":     0,          // è®¾ç½® status å­—æ®µä¸º 0
		}).Error; err != nil {
		logger.Error("SoftDelete: Failed to soft delete file in DB", zap.Error(err), zap.Uint64("fileID", fileID))
		return fmt.Errorf("failed to soft delete file: %w", err)
	}

	ctx := context.Background()
	pipe := r.cache.TxPipeline()
	fileMetadataKey := cache.GenerateFileMetadataKey(file.ID)

	fileMap, err := mapper.FileToMap(file)
	if err != nil {
		logger.Error("FindByID: Failed to map models.File to hash for caching", zap.Uint64("id", file.ID), zap.Error(err))
	} else {
		// HMSet æ›´æ–° file:metadata:<file_id> ä¸­çš„ status å’Œ deleted_at å­—æ®µ
		// å› ä¸ºè½¯åˆ é™¤ä¼šæ›´æ–° DeletedAt å­—æ®µï¼Œæ‰€ä»¥é‡æ–°å­˜å‚¨æ•´ä¸ª map
		pipe.HMSet(ctx, fileMetadataKey, fileMap)
		pipe.Expire(ctx, fileMetadataKey, cache.CacheTTL+time.Duration(rand.Intn(300))*time.Second)
	}

	//ä»åŸæœ¬åˆ—è¡¨ä¸­ç§»é™¤è¯¥æ–‡ä»¶ ID
	listCacheKey := cache.GenerateFileListKey(file.UserID, file.ParentFolderID)
	pipe.ZRem(ctx, listCacheKey, strconv.FormatUint(file.ID, 10))

	// ä½¿ç”¨ deleted_at çš„ Unix æ—¶é—´æˆ³ä½œä¸º Score
	deletedListCacheKey := cache.GenerateDeletedFilesKey(file.UserID)
	if file.DeletedAt.Valid { // ç¡®ä¿ DeletedAt æ˜¯æœ‰æ•ˆçš„
		deletedZMember := &redis.Z{
			Score:  float64(file.DeletedAt.Time.Unix()),
			Member: strconv.FormatUint(file.ID, 10),
		}
		pipe.ZAdd(ctx, deletedListCacheKey, deletedZMember)
		pipe.ZRem(ctx, deletedListCacheKey, "__EMPTY_LIST__") // å¦‚æœä¹‹å‰æœ‰ç©ºæ ‡è®°ï¼Œåˆ é™¤
	} else {
		logger.Warn("SoftDelete: file.DeletedAt is not valid after GORM Delete. Check model hooks.", zap.Uint64("fileID", file.ID))
	}

	// åˆ é™¤å•æ–‡ä»¶ MD5 ç¼“å­˜ï¼Œå› ä¸ºæ–‡ä»¶çŠ¶æ€å˜åŒ–å¯èƒ½å½±å“å…¶æŸ¥æ‰¾
	if file.MD5Hash != nil && *file.MD5Hash != "" {
		pipe.Del(ctx, cache.GenerateFileMD5Key(*file.MD5Hash))
	}

	// æ‰§è¡Œç®¡é“å‘½ä»¤
	if _, execErr := pipe.Exec(ctx); execErr != nil {
		logger.Error("SoftDelete: Failed to execute Redis pipeline for cache update",
			zap.Uint64("fileID", file.ID),
			zap.Uint64("userID", file.UserID),
			zap.Error(execErr),
		)
	}

	logger.Info("SoftDelete: File soft deleted and cache updated", zap.Uint64("fileID", file.ID), zap.Uint64("userID", file.UserID))
	return nil
}

// æ°¸ä¹…åˆ é™¤æ–‡ä»¶
func (r *fileRepository) PermanentDelete(fileID uint64) error {
	file, err := r.FindByID(fileID) // è·å–æ–‡ä»¶ä¿¡æ¯ä»¥ä¾¿åˆ é™¤ç›¸å…³ç¼“å­˜
	if err != nil {
		return err
	}
	if file == nil {
		return xerr.ErrFileNotFound
	}

	// æ°¸ä¹…åˆ é™¤æ•°æ®åº“è®°å½•
	err = r.db.Unscoped().Delete(&models.File{}, fileID).Error // Unscoped() ç»•è¿‡è½¯åˆ é™¤
	if err != nil {
		return fmt.Errorf("failed to permanently delete file: %w", err)
	}

	ctx := context.Background()
	pipe := r.cache.TxPipeline()
	fileMetadataKey := cache.GenerateFileMetadataKey(file.ID)

	//æŠŠfileç»“æ„ä½“æ˜ å°„åˆ°mapç±»å‹
	fileMap, err := mapper.FileToMap(file)
	if err != nil {
		logger.Error("FindByID: Failed to map models.File to hash for caching", zap.Uint64("id", file.ID), zap.Error(err))
	} else {
		// HMSet æ›´æ–° file:metadata:<file_id> ä¸­çš„ status å’Œ deleted_at å­—æ®µ
		// å› ä¸ºè½¯åˆ é™¤ä¼šæ›´æ–° DeletedAt å­—æ®µï¼Œæ‰€ä»¥é‡æ–°å­˜å‚¨æ•´ä¸ª map
		pipe.HMSet(ctx, fileMetadataKey, fileMap)
		pipe.Expire(ctx, fileMetadataKey, cache.CacheTTL+time.Duration(rand.Intn(300))*time.Second)
	}

	// Del åˆ é™¤ file:metadata:<file_id> å“ˆå¸Œé”®
	pipe.Del(ctx, cache.GenerateFileMetadataKey(file.ID))

	// ZRem ä»æ‰€æœ‰å¯èƒ½å­˜åœ¨çš„ç›¸å…³ Sorted Set ä¸­ç§»é™¤è¯¥æ–‡ä»¶ ID
	// ä»åŸçˆ¶ç›®å½•çš„ Sorted Set ä¸­ç§»é™¤ (æ— è®ºå®ƒæ˜¯å¦åœ¨å›æ”¶ç«™ï¼ŒåŸçˆ¶ç›®å½•åˆ—è¡¨éƒ½ä¸åº”å†åŒ…å«å®ƒ)
	listCacheKey := cache.GenerateFileListKey(file.UserID, file.ParentFolderID)
	pipe.ZRem(ctx, listCacheKey, strconv.FormatUint(file.ID, 10))

	// ä»å›æ”¶ç«™åˆ—è¡¨ Sorted Set ä¸­ç§»é™¤ (å¦‚æœå®ƒåœ¨å›æ”¶ç«™çš„è¯)
	deletedListCacheKey := cache.GenerateDeletedFilesKey(file.UserID)
	pipe.ZRem(ctx, deletedListCacheKey, strconv.FormatUint(file.ID, 10))

	if file.MD5Hash != nil && *file.MD5Hash != "" {
		pipe.Del(ctx, cache.GenerateFileMD5Key(*file.MD5Hash))
	}

	// æ‰§è¡Œç®¡é“å‘½ä»¤
	if _, execErr := pipe.Exec(ctx); execErr != nil {
		logger.Error("PermanentDelete: Failed to execute Redis pipeline for cache update",
			zap.Uint64("fileID", file.ID),
			zap.Uint64("userID", file.UserID),
			zap.Error(execErr),
		)
	}

	logger.Info("PermanentDelete: File permanently deleted and cache invalidated", zap.Uint64("fileID", file.ID), zap.Uint64("userID", file.UserID))
	return nil
}

// GetChildrenByPathPrefix è·å–æ‰€æœ‰ä»¥ç»™å®šè·¯å¾„å‰ç¼€å¼€å¤´çš„å­é¡¹ (ç”¨äºæ›´æ–° Path å­—æ®µ) (æœªä½¿ç”¨)
func (r *fileRepository) FindChildrenByPathPrefix(userID uint64, pathPrefix string) ([]models.File, error) {
	var files []models.File
	err := r.db.Where("user_id = ? AND path LIKE ?", userID, pathPrefix+"%").Find(&files).Error
	if err != nil {
		return nil, err
	}
	return files, nil
}

// UpdateFilesPathInBatch æ‰¹é‡æ›´æ–°æ–‡ä»¶çš„ Path å­—æ®µ
// å¼‚æ­¥ç¼“å­˜å¤±æ•ˆ
// æ€è·¯ï¼šåœ¨æ‰¹é‡æ›´æ–°æ•°æ®åº“åï¼Œä¸ç«‹å³åŒæ­¥åˆ é™¤ Redis ç¼“å­˜ï¼Œ
// è€Œæ˜¯å‘é€ä¸€ä¸ªæ¶ˆæ¯åˆ°æ¶ˆæ¯é˜Ÿåˆ—ï¼ˆå¦‚ RabbitMQï¼‰ï¼Œç”±ä¸€ä¸ªç‹¬ç«‹çš„æ¶ˆè´¹è€…è¿›ç¨‹å¼‚æ­¥åœ°å»å¤„ç†ç¼“å­˜å¤±æ•ˆé€»è¾‘ã€‚
func (r *fileRepository) UpdateFilesPathInBatch(userID uint64, oldPathPrefix, newPathPrefix string) error {
	// ä½¿ç”¨ REPLACE SQL å‡½æ•°è¿›è¡Œå­—ç¬¦ä¸²æ›¿æ¢
	if err := r.db.Model(&models.File{}).
		Where("user_id = ? AND path LIKE ?", userID, oldPathPrefix+"%").
		Update("path", gorm.Expr("REPLACE(path, ?, ?)", oldPathPrefix, newPathPrefix)).Error; err != nil {
		return err
	}

	//æ•°æ®åº“æ›´æ–°æˆåŠŸå,å‘é€ç¼“å­˜å¤±æ•ˆä¿¡æ¯
	message := cache.CachePathInvalidationMessage{
		UserID:        userID,
		OldPathPrefix: oldPathPrefix,
		NewPathPrefix: newPathPrefix,
	}
	messageJSON, _ := json.Marshal(message)

	_, err := r.cache.XAdd(context.Background(), &redis.XAddArgs{
		Stream: "cache_path_invalidation_stream",
		MaxLen: 10000,
		Values: map[string]any{"payload": messageJSON},
	}).Result()

	if err != nil {
		// æ¶ˆæ¯å‘é€å¤±è´¥ä¸è¿”å›é”™è¯¯ï¼Œä½†éœ€è®°å½•æ—¥å¿—å¹¶å‘Šè­¦
		logger.Error("Failed to publish cache path invalidation message", zap.Error(err))
	}
	return nil
}

func (r *fileRepository) UpdateFileStatus(fileID uint64, status uint8) error {
	// æ›´æ–°æ•°æ®åº“
	if err := r.db.Model(&models.File{}).Where("id = ?", fileID).Update("status", status).Error; err != nil {
		logger.Error("UpdateFileStatus: Failed to update file status in DB", zap.Uint64("fileID", fileID), zap.Uint8("status", status), zap.Error(err))
		return fmt.Errorf("failed to update file status: %w", err)
	}

	// å¼‚æ­¥ç¼“å­˜æ›´æ–°
	ctx := context.Background()
	file, err := r.FindByID(fileID)
	if err != nil {
		logger.Error("UpdateFileStatus: Failed to find file for cache invalidation", zap.Uint64("fileID", fileID), zap.Error(err))
		return nil // æ•°æ®åº“å·²æ›´æ–°ï¼Œç¼“å­˜é—®é¢˜ä¸åº”é˜»å¡ä¸»æµç¨‹
	}

	message := cache.CacheUpdateMessage{
		File: *file,
	}
	messageJSON, _ := json.Marshal(message)

	_, streamErr := r.cache.XAdd(ctx, &redis.XAddArgs{
		Stream: "file_cache_updates",
		MaxLen: 10000,
		Values: map[string]any{"payload": messageJSON},
	}).Result()

	if streamErr != nil {
		logger.Error("UpdateFileStatus: Failed to publish cache update message", zap.Uint64("fileID", fileID), zap.Error(streamErr))
	}

	return nil
}

// CountFilesInStorage æ ¹æ® OssKey å’Œ MD5Hash æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å­˜åœ¨é™¤ç»™å®š fileID ä¹‹å¤–çš„å…¶ä»–æ–‡ä»¶è®°å½•
// è¿”å›å¼•ç”¨è¯¥ OssKey çš„æ–‡ä»¶æ•°é‡ (åŒ…æ‹¬è‡ªèº«ï¼Œä½†ä¸åŒ…æ‹¬å·²é€»è¾‘åˆ é™¤çš„æˆ–æ­£åœ¨è¢«åˆ é™¤çš„)
// ä¼ å…¥ currentDeletingFileID æ˜¯ä¸ºäº†åœ¨è®¡ç®—å¼•ç”¨æ•°æ—¶æ’é™¤å½“å‰æ­£åœ¨æ°¸ä¹…åˆ é™¤çš„æ–‡ä»¶ï¼Œé¿å…è®¡ç®—é”™è¯¯ã€‚
// åœ¨æœ¬å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬åº”è¯¥è®¡ç®—æ‰€æœ‰"æ­£å¸¸"çŠ¶æ€çš„å¼•ç”¨ã€‚
func (r *fileRepository) CountFilesInStorage(ossKey string, md5Hash string, excludeFileID uint64) (int64, error) {
	var count int64
	// æŸ¥æ‰¾æ‰€æœ‰ status = 1 (æ­£å¸¸) çš„æ–‡ä»¶è®°å½•ï¼Œä¸” OssKey å’Œ MD5Hash åŒ¹é…
	// åŒæ—¶æ’é™¤å½“å‰æ­£åœ¨è¢«æ°¸ä¹…åˆ é™¤çš„æ–‡ä»¶è®°å½•æœ¬èº«
	err := r.db.Model(&models.File{}).
		Where("oss_key = ? AND md5_hash = ? AND status = 1 AND id != ?", ossKey, md5Hash, excludeFileID).
		Count(&count).Error
	if err != nil {
		logger.Error("Failed to count files in storage for ossKey",
			zap.String("ossKey", ossKey),
			zap.String("md5Hash", md5Hash),
			zap.Uint64("excludeFileID", excludeFileID),
			zap.Error(err))
		return 0, fmt.Errorf("failed to count files in storage: %w", err)
	}
	return count, nil
}

// getFilesFromCacheList æ˜¯ä¸€ä¸ªç§æœ‰çš„è¾…åŠ©å‡½æ•°ï¼Œç”¨äºä» Redis Sorted Set ç¼“å­˜ä¸­è·å–æ–‡ä»¶ ID åˆ—è¡¨ï¼Œ
// å¹¶æ‰¹é‡ä» Hash ç¼“å­˜ä¸­è·å–å¯¹åº”çš„æ–‡ä»¶å…ƒæ•°æ®ï¼Œæœ€åååºåˆ—åŒ–ä¸º []models.Fileã€‚
// å®ƒå¤„ç†äº†ç©ºåˆ—è¡¨æ ‡è®°å’Œç¼“å­˜è¯»å–é”™è¯¯ã€‚
func (r *fileRepository) getFilesFromCacheList(ctx context.Context, listCacheKey string) ([]models.File, error) {
	keyExists, err := r.cache.Exists(ctx, listCacheKey)
	if err != nil {
		logger.Error("getFilesFromCacheList: Error checking key existence in cache", zap.String("listCacheKey", listCacheKey), zap.Error(err))
		return nil, fmt.Errorf("failed to check cache key existence: %w", err)
	}

	if !keyExists { // é”®ä¸å­˜åœ¨
		logger.Debug("getFilesFromCacheList: Cache miss - list key not found (Exists returned 0)", zap.String("listCacheKey", listCacheKey))
		return nil, cache.ErrCacheMiss // æ˜ç¡®è¿”å›ç¼“å­˜æœªå‘½ä¸­
	}

	// 1. ä» Redis Sorted Set è·å–æ–‡ä»¶ ID åˆ—è¡¨
	fileIDsStr, err := r.cache.ZRevRange(ctx, listCacheKey, 0, -1).Result()
	if err != nil {
		if err == redis.Nil { // åˆ—è¡¨ Key ä¸å­˜åœ¨ï¼Œè§†ä¸ºç¼“å­˜æœªå‘½ä¸­
			logger.Info("getFilesFromCacheList: Cache miss - list key not found", zap.String("listCacheKey", listCacheKey))
			return nil, cache.ErrCacheMiss // è¿”å›è‡ªå®šä¹‰çš„ç¼“å­˜æœªå‘½ä¸­é”™è¯¯
		}
		logger.Error("Error getting file ID list from cache", zap.String("key", listCacheKey), zap.Error(err))
		return nil, fmt.Errorf("failed to get file ID list from cache: %w", err)
	}

	if len(fileIDsStr) == 0 {
		// è¿™æ˜¯ä¸€ä¸ªå­˜åœ¨çš„ç©º Sorted Setï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ __EMPTY_LIST__ æ ‡è®°
		// å› ä¸º ZRevRange å·²ç»è¿”å›ç©ºï¼Œæ‰€ä»¥è¿™é‡Œä¸éœ€è¦å† ZRange ç¡®è®¤äº†
		// å¦‚æœ saveFilesToCacheList å†™å…¥äº† __EMPTY_LIST__ï¼ŒZRevRange ä¸ä¼šè¿”å›ç©ºåˆ‡ç‰‡
		// æ‰€ä»¥èµ°åˆ°è¿™é‡Œï¼Œè¯´æ˜æ˜¯å­˜åœ¨ä¸€ä¸ªç©ºçš„ Sorted Setï¼Œä¸”æ²¡æœ‰ __EMPTY_LIST__ æ ‡è®°
		logger.Warn("getFilesFromCacheList: Sorted Set exists but is truly empty (no members, no __EMPTY_LIST__ marker). Treating as cache miss to force DB refresh.", zap.String("listCacheKey", listCacheKey))
		return nil, cache.ErrCacheMiss // å¼ºåˆ¶è§†ä¸ºç¼“å­˜æœªå‘½ä¸­ï¼Œå›æº
	}

	// æ£€æŸ¥æ˜¯å¦æ˜¯ç©ºåˆ—è¡¨æ ‡è®° (é˜²æ­¢ç¼“å­˜ç©¿é€çš„ç‰¹æ®Šæ ‡è®°)
	if len(fileIDsStr) == 1 {
		if fileIDsStr[0] == "__EMPTY_LIST__" {
			return []models.File{}, nil // è¿”å›ç©ºåˆ‡ç‰‡
		}
	}

	// è½¬æ¢æ–‡ä»¶ ID å­—ç¬¦ä¸²ä¸º uint64
	var fileIDs []uint64
	for _, idStr := range fileIDsStr {
		id, parseErr := strconv.ParseUint(idStr, 10, 64)
		if parseErr != nil {
			logger.Error("Failed to parse file ID from cache", zap.String("idStr", idStr), zap.Error(parseErr))
			continue // è·³è¿‡æ— æ•ˆ ID
		}
		if id > 0 { // ç¡®ä¿ ID æœ‰æ•ˆ
			fileIDs = append(fileIDs, id)
		}
	}

	// å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ–‡ä»¶IDï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
	if len(fileIDs) == 0 {
		return []models.File{}, nil
	}

	// 2. æ‰¹é‡ä» Hash (file:metadata:<id>) ä¸­è·å–æ¯ä¸ªæ–‡ä»¶çš„å…ƒæ•°æ®
	pipe := r.cache.TxPipeline()
	cmds := make(map[uint64]*redis.StringStringMapCmd)
	for _, fileID := range fileIDs {
		metaKey := fmt.Sprintf("file:metadata:%d", fileID)
		cmds[fileID] = pipe.HGetAll(ctx, metaKey)
	}

	_, execErr := pipe.Exec(ctx)
	if execErr != nil && execErr != redis.Nil {
		logger.Error("Error executing HGetAll pipeline for files metadata", zap.Error(execErr))
		return nil, fmt.Errorf("failed to execute HGetAll pipeline: %w", execErr)
	}

	// å¤„ç†ç®¡é“ç»“æœå¹¶ååºåˆ—åŒ–
	var files []models.File
	for _, fileID := range fileIDs {
		cmd := cmds[fileID]
		fileMap, getErr := cmd.Result()
		if getErr == nil && len(fileMap) > 0 {
			// å¿½ç•¥ç©ºå€¼æ ‡è®°çš„å“ˆå¸Œ
			if _, ok := fileMap["__NOT_FOUND__"]; !ok {
				file, mapErr := mapper.MapToFile(fileMap)
				if mapErr == nil {
					files = append(files, *file)
				} else {
					logger.Error("Failed to map file hash to struct from cache", zap.Uint64("fileID", fileID), zap.Error(mapErr))
					// è®°å½•é”™è¯¯ä½†ä¸é˜»æ­¢å…¶ä»–æ–‡ä»¶è¢«å¤„ç†
				}
			}
		} else if getErr != nil && getErr != redis.Nil {
			logger.Error("Error getting file metadata hash for ID", zap.Uint64("fileID", fileID), zap.Error(getErr))
			// è®°å½•é”™è¯¯ä½†ä¸é˜»æ­¢å…¶ä»–æ–‡ä»¶è¢«å¤„ç†
		}
	}

	return files, nil
}

// ç”¨äºå°† []models.File åˆ—è¡¨å­˜å‚¨åˆ° Redis ç¼“å­˜ä¸­ã€‚
// å®ƒå°†æ–‡ä»¶å…ƒæ•°æ®å­˜å‚¨ä¸º Hashï¼Œå¹¶å°†æ–‡ä»¶ ID å­˜å‚¨åˆ° Sorted Setã€‚
func (r *fileRepository) saveFilesToCacheList(ctx context.Context, cacheKey string, files []models.File, scoreFunc func(file models.File) float64) error {
	pipe := r.cache.TxPipeline()

	if len(files) == 0 {
		// å¦‚æœåˆ—è¡¨ä¸ºç©ºï¼Œå­˜ä¸€ä¸ªç©ºåˆ—è¡¨æ ‡è®°ï¼Œé˜²æ­¢ç¼“å­˜ç©¿é€
		pipe.ZAdd(ctx, cacheKey, &redis.Z{Score: 0, Member: "__EMPTY_LIST__"})
		logger.Info("successfully zadd cacheKey member:__EMPTY_LIST__")
	} else {
		var zs []*redis.Z
		for _, file := range files {
			// å­˜å‚¨æ–‡ä»¶å…ƒæ•°æ®åˆ° Hash
			fileMap, mapErr := mapper.FileToMap(&file)
			if mapErr != nil {
				logger.Error("saveFilesToCacheList: Failed to map models.File to hash for caching", zap.Uint64("fileID", file.ID), zap.Error(mapErr))
				continue // è®°å½•é”™è¯¯ä½†ä¸é˜»æ­¢å…¶ä»–æ–‡ä»¶è¢«ç¼“å­˜
			}
			metaKey := cache.GenerateFileMetadataKey(file.ID)
			pipe.HMSet(ctx, metaKey, fileMap)
			pipe.Expire(ctx, metaKey, cache.CacheTTL+time.Duration(rand.Intn(300))*time.Second) // Hash ä¹Ÿè¦è®¾ç½® TTL

			// å‡†å¤‡ Sorted Set æˆå‘˜ï¼šä½¿ç”¨ä¼ å…¥çš„ scoreFunc è®¡ç®— Score
			zs = append(zs, &redis.Z{
				Score:  scoreFunc(file),
				Member: strconv.FormatUint(file.ID, 10),
			})
		}
		if len(zs) > 0 {
			pipe.ZAdd(ctx, cacheKey, zs...) // æ·»åŠ æ‰€æœ‰æ–‡ä»¶ ID åˆ° Sorted Set
		}
	}
	pipe.Expire(ctx, cacheKey, cache.CacheTTL+time.Duration(rand.Intn(300))*time.Second) // è®¾ç½®åˆ—è¡¨çš„ TTL

	// æ‰§è¡Œæ‰€æœ‰ç®¡é“å‘½ä»¤
	_, execErr := pipe.Exec(ctx)
	if execErr != nil {
		// ç¼“å­˜å›å¡«é€»è¾‘ï¼Œä¸åº”é˜»å¡ç”¨æˆ·è·å–æ•°æ®çš„è¯·æ±‚ã€‚
		// å¯ä»¥é€‰æ‹©å¿½ç•¥è¿™ä¸ªé”™è¯¯ï¼Œè®©ä¸‹æ¬¡çš„è¯»è¯·æ±‚é‡æ–°å›æºæ•°æ®åº“ï¼Œç¡®ä¿ç”¨æˆ·æ‹¿åˆ°æ­£ç¡®æ•°æ®ã€‚
		logger.Error("saveFilesToCacheList: Failed to execute Redis pipeline for caching list. Cache might be inconsistent.", zap.String("key", cacheKey), zap.Error(execErr))
	}
	return nil
}
